# dockerized-LLM
A dockerized version of LLaMA or MPT30B model so that it can be inferred as a web service using fastAPI
